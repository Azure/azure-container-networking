parameters:
  clusterName: ""
  os: "linux"
  restartCase: "false"
  cni: "cilium"
  cnsManagedEndpoint: false

steps:
  - task: AzureCLI@1
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        if [ ${{ parameters.cnsManagedEndpoint }} == "true" ] && [ ${{ parameters.restartCase }} == "true"]
        then
          echo If this step fails, test manually.
          echo Only fails when CNS is managing the endpoint state and state file does not existing due to node restart with 0 pods, outside of kube-system pods, scheduled on node.
          echo Failure is due to slow node restart which occurs after scale down. This also misses the intent of the restart node scenario.
          echo Timing should be: Scale down > Restart nodes during scale down > complete scale down > validate.
          echo Contnuing on Error only when endpoint state is managed by CNS and RestartCase == True to allow for further test cases to run
        fi

        make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}
        kubectl get pods -A
        make test-validate-state OS=${{ parameters.os }} RESTART_CASE=${{ parameters.restartCase }} CNI_TYPE=${{ parameters.cni }}
    name: "ValidateState"
    displayName: "Validate State"
    retryCountOnTaskFailure: 3
    continueOnError: ${{ parameters.cnsManagedEndpoint }}
