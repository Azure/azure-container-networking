parameters:
  dependsOn: ""
  name: ""
  clusterType: ""
  clusterName: ""
  nodeCount: ""
  vmSize: ""
  os: linux
  arch: ""
  osSKU: Ubuntu

# CNIv2
# + Should be able to scale up/down the pods successfully certain number of times.
# + Node reboot scenarios should be covered.
# + The CNS state should be validated with that of CNI state.
# + Pods should have ips assigned and connectivity/datapath test should be present.
# + CNS restart and validates the state
# Windows
# The HNS state should be validated with that of CNI state.
# + All CNI E2E is re-ran after HNS service is restarted

stages:
  - stage: create_${{ parameters.name }}
    variables:
      commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
    pool:
      name: $(BUILD_POOL_NAME_DEFAULT)
    dependsOn:
      - setup
    displayName: "Create Cluster - ${{ parameters.clusterName }}"
    jobs:
      - job: create_aks_cluster_with_${{ parameters.name }}
        steps:
          - task: AzureCLI@1
            inputs:
              azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
              scriptLocation: "inlineScript"
              scriptType: "bash"
              addSpnToEnvironment: true
              inlineScript: |
                make -C /aks/hack ${{ parameters.clusterType }} \
                AZCLI=az REGION=$(LOCATION) SUB=$(SUB_AZURE_NETWORK_AGENT_BUILD_VALIDATIONS) \
                CLUSTER=${{ parameters.clusterName }}-$(commitID) \
                VM_SIZE=${{ parameters.vmSize }} \
                AUTOUPGRADE=none
            name: "CreateAksCluster"
            displayName: "Create AKS Cluster"

  - stage: ${{ parameters.name }}
    variables:
      commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
      cnsVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.cnsVersion'] ]
      cniVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.cniVersion'] ]
    pool:
      name: $(BUILD_POOL_NAME_DEFAULT)
    dependsOn:
      - create_${{ parameters.name }}
      - setup
    displayName: "CNIv2 Test - ${{ parameters.name }}"
    jobs:
      - job: integration
        displayName: "Integration Test - ${{ parameters.name }}"
        steps:
          - task: AzureCLI@1
            inputs:
              azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
              scriptLocation: "inlineScript"
              scriptType: "bash"
              addSpnToEnvironment: true
              inlineScript: |
                echo cns version - $(cnsVersion)
                echo cni version - $(cniVersion)

                echo "Start Integration Tests on Overlay Cluster"
                make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
                kubectl cluster-info
                kubectl get po -owide -A
                sudo -E env "PATH=$PATH" make test-integration CNS_VERSION=$(cnsVersion) CNI_VERSION=$(cniVersion) INSTALL_CNS=true INSTALL_AZURE_CNI_OVERLAY=true CNS_IMAGE_REPO=MCR CNI_IMAGE_REPO=MCR
            name: "overlaye2e"
            displayName: "Overlay Integration"
      - job: kernel_upgrade
        displayName: "Scale Test"
        dependsOn: npm_k8se2e
        steps:
          - task: AzureCLI@1
            inputs:
              azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
              scriptLocation: "inlineScript"
              scriptType: "bash"
              addSpnToEnvironment: true
              inlineScript: |
                set -ex
                az extension add --name aks-preview
                make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
            retryCountOnTaskFailure: 3
            name: "config"
            displayName: "Set Kubeconfig"
          - script: |
              set -e

              echo "-- Start privileged daemonset --"
              kubectl get pods -Aowide
              kubectl apply -f test/integration/manifests/load/privileged-daemonset.yaml
              sleep 10s
              kubectl rollout status ds -n kube-system privileged-daemonset
            retryCountOnTaskFailure: 3
            name: "priv_daemonset"
            displayName: "Create Daemonset"
          - script: |
              set -e

              kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide
              privList=`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $1}'`
              for pod in $privList; do
                echo "-- Update Ubuntu Packages --"
                # Not needed, but ensures that the correct packages exist to perform upgrade
                kubectl exec -i -n kube-system $pod -- bash -c "apt update && apt-get install software-properties-common -y"
              done
            retryCountOnTaskFailure: 3
            name: "Ubuntu_Setup"
            displayName: "Ubuntu Setup"
          - script: |
              set -e
              echo "-- Update kernel through daemonset --"
              kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide
              privList=`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $1}'`
              for pod in $privList; do
                echo "-- Add proposed repository --"
                kubectl exec -i -n kube-system $pod -- bash -c "add-apt-repository ppa:canonical-kernel-team/proposed -y"
                kubectl exec -i -n kube-system $pod -- bash -c "add-apt-repository ppa:canonical-kernel-team/proposed2 -y"

                echo "-- Check apt-cache --"
                kubectl exec -i -n kube-system $pod -- bash -c "apt-cache madison linux-azure-edge"

                echo "-- Check current Ubuntu kernel --"
                kubectl exec -i -n kube-system $pod -- bash -c "uname -r"
                kubectl get node -owide
              done

              temp=`apt-cache madison linux-azure-edge | grep proposed | awk '{print $3}'`
              for t in $temp; do
                if [[ $t =~ ${KERNEL_VERSION} ]]; then
                  echo "$t - looking for ${KERNEL_VERSION}"
                  flag1=true
                fi
                if [[ $t =~ ${KERNEL_VERSION2} ]]; then
                  echo "$t - looking for ${KERNEL_VERSION2}"
                  flag2=true
                fi
              done
              if [ -z $flag1 ] || [ -z $flag2 ]; then
                echo "Missing required proposed repos"
                exit 1
              fi
            retryCountOnTaskFailure: 3
            name: "Kernel_Setup"
            displayName: "Kernel Setup"
          - script: |
              set -e
              echo "-- Update kernel through daemonset --"
              kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide
              privList=`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $1}'`
              for pod in $privList; do
                echo "-- Install Proposed Kernel --"
                kubectl exec -i -n kube-system $pod -- bash -c "apt install -y linux-azure-edge"
              done

              for pod in $privList; do
                echo "-- Check for Installed Kernel --"
                installed=`kubectl exec -i -n kube-system $pod -- bash -c "apt list linux-azure-edge --installed" | grep installed | awk '{print $4}'`
                if [ -z $installed ]; then
                  echo $pod needs to reinstall kernel
                  exit 1
              done
            retryCountOnTaskFailure: 3
            name: "Kernel_Install"
            displayName: "Kernel Install"
          - script: |
              set -e
              echo "-- Restart Nodes to Finalize Upgrade Kernel --"
              kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide

              privArray=(`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $1}'`)
              nodeArray=(`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $7}'`)
              kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide

              i=0
              for _ in ${privArray[@]}; do
                echo "-- Restarting Node ${nodeArray[i]} through ${privArray[i]} --"
                kubectl exec -i -n kube-system ${privArray[i]} -- bash -c "reboot"
                echo "-- Waiting for condition NotReady --"
                kubectl wait --for=condition=Ready=false -n kube-system pod/${privArray[i]} --timeout=90s
                echo "-- Waiting for condition Ready --"
                kubectl wait --for=condition=Ready -n kube-system pod/${privArray[i]} --timeout=90s
                ((i = i + 1))
                echo "Wait 10s for pods to settle"
                sleep 10s
              done

              privList=`kubectl get pods -n kube-system -l os=linux,app=privileged-daemonset -owide --no-headers | awk '{print $1}'`
              # Add in regex check for expected kernel
              kubectl rollout status ds -n kube-system privileged-daemonset

              for pod in $privList; do
                echo "-- Confirm Ubuntu Kernel --"
                confirm=`kubectl exec -i -n kube-system $pod -- bash -c "uname -r"
                if [[ ! $confirm =~ ${KERNEL_VERSION} ]]; then
                  echo $pod kernel version - $confirm, expected - ${KERNEL_VERSION}
                  exit 1
                fi
              done
              kubectl get node -owide
            retryCountOnTaskFailure: 3
            name: "Kernel_Upgrade"
            displayName: "Kernel Upgrade"
      # - job: deploy_pods
      #   displayName: "Scale Test"
      #   dependsOn: npm_k8se2e
      #   steps:
      #     - template: ../load-test-templates/pod-deployment-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         scaleup: ${SCALEUP_LINUX}
      #         os: ${{ parameters.os }}
      #         iterations: ${ITERATIONS_LINUX}
      #         nodeCount: ${{ parameters.nodeCount }}
      #         cni: cniv2
      #     - template: ../load-test-templates/validate-state-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         os: ${{ parameters.os }}
      #         cni: cniv2
      # - job: restart_nodes
      #   displayName: "Restart Test"
      #   dependsOn: deploy_pods
      #   steps:
      #     - template: ../load-test-templates/restart-node-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         os: ${{ parameters.os }}
      #         cni: cniv2
      #     - template: ../load-test-templates/validate-state-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         os: ${{ parameters.os }}
      #         cni: cniv2
      #         restartCase: "true"
      # - job: restart_cns
      #   displayName: "Restart and Validate CNS"
      #   dependsOn: restart_nodes
      #   steps:
      #     - template: ../load-test-templates/restart-cns-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         os: ${{ parameters.os }}
      #         cni: cniv2
      #         scaleup: ${SCALEUP_LINUX}
      #         nodeCount: ${{ parameters.nodeCount }}
      # - job: recover
      #   displayName: "Recover Resources"
      #   dependsOn: restart_cns
      #   steps:
      #     - task: AzureCLI@1
      #       inputs:
      #         azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      #         scriptLocation: "inlineScript"
      #         scriptType: "bash"
      #         addSpnToEnvironment: true
      #         inlineScript: |
      #           echo "Delete load-test Namespace"
      #           make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
      #           kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'
      #           delete=`kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'`
      #           kubectl delete ns $delete
      #           kubectl cluster-info
      #           kubectl get po -owide -A
      #       name: "recover"
      #       displayName: "Delete test Namespaces"
      # - template: ../k8s-e2e/k8s-e2e-job-template.yaml
      #   parameters:
      #     sub: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      #     clusterName: ${{ parameters.clusterName }}-$(commitID)
      #     os: ${{ parameters.os }}
      #     dependsOn: recover
      #     datapath: true
      #     dns: true
      #     portforward: true
      #     service: true
      #     hostport: true
      # - job: failedE2ELogs
      #   displayName: "Failure Logs"
      #   dependsOn:
      #     - integration
      #     - deploy_pods
      #     - restart_nodes
      #     - restart_cns
      #     - recover
      #     - cni_${{ parameters.os }}
      #   condition: failed()
      #   steps:
      #     - template: ../../templates/log-template.yaml
      #       parameters:
      #         clusterName: ${{ parameters.clusterName }}-$(commitID)
      #         os: ${{ parameters.os }}
      #         cni: cniv2
