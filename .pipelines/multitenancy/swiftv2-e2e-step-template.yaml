parameters:
  name: ""
  clusterName: ""
  continueOnError: true
  arch: ""
  os: ""
  scenario: ""
  region: ""

steps:
  - bash: |
      go version
      go env
      mkdir -p '$(GOBIN)'
      mkdir -p '$(GOPATH)/pkg'
      mkdir -p '$(modulePath)'
      echo '##vso[task.prependpath]$(GOBIN)'
      echo '##vso[task.prependpath]$(GOROOT)/bin'
    name: "GoEnv"
    displayName: "Set up the Go environment"

  - task: KubectlInstaller@0
    inputs:
      kubectlVersion: latest

  - task: AzureCLI@2
    inputs:
      azureSubscription: $(ACN_TEST_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -ex
        source ./.pipelines/multitenancy/scripts/utils.sh
        export_envVars ${{ parameters.scenario }} 
        make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }} GROUP=$RG
    name: "kubeconfig"
    displayName: "Set Kubeconfig"

  - task: AzureCLI@2
    inputs:
      azureSubscription: $(ACN_TEST_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -ex
        source ./.pipelines/multitenancy/scripts/utils.sh
        export_envVars ${{ parameters.scenario }} 
        make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }} GROUP=$RG
        echo "Upload CNI"
        echo "Deploying on Linux nodes"
        export CNI_IMAGE=$(make cni-image-name-and-tag OS=${{ parameters.os }} ARCH=${{ parameters.arch }} CNI_VERSION=$(make cni-version))
        echo "CNI image: $CNI_IMAGE"
        envsubst '${CNI_IMAGE}' < ./test/integration/manifests/cni/cni-installer-v1.yaml | kubectl apply -f -
        kubectl rollout status daemonset/azure-cni -n kube-system
        kubectl get pods -A -owide
    name: "deployCNI"
    displayName: "Deploy CNI"
  - task: AzureCLI@2
    inputs:
      azureSubscription: $(ACN_TEST_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -ex
        source ./.pipelines/multitenancy/scripts/utils.sh
        export_envVars ${{ parameters.scenario }} 
        make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }} GROUP=$RG
        echo "Keep CNS version up to date, grabbing pipeline parameter"
        CNS_IMAGE=$(make cns-image-name-and-tag)
        sed -i '/containers:/{n;n;s/\(image\).*/\1: '"${CNS_IMAGE//\//\\/}"'/}' ./test/integration/manifests/cns/daemonset-linux.yaml
        kubectl apply -f ./test/integration/manifests/cns/daemonset-linux.yaml
        for val in $(az vmss list -g MC_${{ parameters.clusterName }}_${{ parameters.clusterName }}_$(REGION_SWIFTV2_CLUSTER_TEST) --query "[].name" -o tsv); do
          make -C ./hack/aks restart-vmss AZCLI=az CLUSTER=${{ parameters.clusterName }} GROUP=$RG REGION=$(REGION_SWIFTV2_CLUSTER_TEST) VMSS_NAME=${val}
        done
        kubectl get node
        kubectl rollout status daemonset/azure-cns -n kube-system
        kubectl get pod -A
    name: "UpdateCNSVersion"
    displayName: "Update CNS Version"

  - task: AzureCLI@2
    name: runSwiftv2Tests
    displayName: "Run Tests"
    inputs:
      azureSubscription: $(ACN_TEST_SERVICE_CONNECTION)
      addSpnToEnvironment: true
      scriptType: "bash"
      scriptLocation: "inlineScript"
      inlineScript: |
        set -xe

        bash .pipelines/multitenancy/scripts/az-login.sh \
          --service-principal "$servicePrincipalId" \
          --id-token "$idToken" \
          --tenant "$tenantId"

        bash .pipelines/multitenancy/scripts/run-tests.sh \
          --test-dir "$(System.DefaultWorkingDirectory)/.pipelines/multitenancy/scripts" \
          --mt-test-cluster ${{ parameters.clusterName }} \
          --scenario ${{ parameters.scenario }}
                
  # - task: AzureCLI@2
  #   inputs:
  #     azureSubscription: $(TEST_SUB_SERVICE_CONNECTION)
  #     scriptLocation: "inlineScript"
  #     scriptType: "bash"
  #     addSpnToEnvironment: true
  #     inlineScript: |
  #       ls -lah
  #       pwd
  #       kubectl cluster-info
  #       kubectl get po -owide -A
  #       echo "Apply the pod network yaml to start the delegation"
  #       less test/integration/manifests/swiftv2/podnetwork.yaml
  #       envsubst '${SUBNET_TOKEN},${SUBNET_RESOURCE_ID},${VNET_GUID}' < test/integration/manifests/swiftv2/podnetwork.yaml | kubectl apply -f -
  #       echo "Check the podnetwork yaml file"
  #       less test/integration/manifests/swiftv2/podnetwork.yaml
  #       kubectl get pn
  #       kubectl describe pn
  #       echo "Apply the pod network instance yaml to reserve IP"
  #       kubectl apply -f test/integration/manifests/swiftv2/pni.yaml
  #       kubectl get pni
  #       kubectl describe pni
  #       export NODE_NAME_0="$(kubectl get nodes -o json | jq -r .items[0].metadata.name)"
  #       echo $NODE_NAME_0
  #       echo "Start the first pod using the reserved IP"
  #       envsubst '$NODE_NAME_0' < test/integration/manifests/swiftv2/mtpod0.yaml | kubectl apply -f -
  #       export NODE_NAME_1="$(kubectl get nodes -o json | jq -r .items[1].metadata.name)"
  #       echo $NODE_NAME_1
  #       echo "Start another pod using the reserved IP"
  #       envsubst '$NODE_NAME_1' < test/integration/manifests/swiftv2/mtpod1.yaml | kubectl apply -f -
  #       sleep 2m
  #       kubectl get pod -o wide -A
  #       sleep 2m
  #       echo "Check pods after 4 minutes"
  #       kubectl get po -owide -A
  #       kubectl describe pni
  #   name: "start_swiftv2_pods"
  #   displayName: "Start Swiftv2 Pods"
  #   continueOnError: ${{ parameters.continueOnError }}
  #   env:
  #     SUBNET_TOKEN: $(SUBNET_TOKEN)

  # - script: |
  #     set -e
  #     kubectl get po -owide -A
  #     cd test/integration/swiftv2
  #     echo "TestSwiftv2PodToPod and will run it after migration from scripts."
  #     go test -count=1 swiftv2_test.go -timeout 3m -tags swiftv2 -run ^TestSwiftv2PodToPod$ -tags=swiftv2,integration -v
  #   retryCountOnTaskFailure: 3
  #   name: "Swiftv2_Tests_future_version"
  #   displayName: "Swiftv2 Tests through code"
  #   continueOnError: ${{ parameters.continueOnError }}

