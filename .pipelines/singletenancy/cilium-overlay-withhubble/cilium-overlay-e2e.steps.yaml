parameters:
  name: ""
  clusterName: ""
  testHubble: false
  scaleup: ""

steps:
  - task: KubectlInstaller@0
    inputs:
      kubectlVersion: latest

  - task: AzureCLI@2
    displayName: "Install Cilium on AKS Overlay"
    env:
      AZCLI: az
      CLUSTER: ${{ parameters.clusterName }}
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        make -C ./hack/aks set-kubeconf
        ls -lah
        export CILIUM_VERSION_TAG=${CILIUM_HUBBLE_VERSION_TAG}
        export DIR=$(echo ${CILIUM_VERSION_TAG#v} | cut -d. -f1,2)
        echo "installing files from ${DIR}"
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-config/cilium-config-hubble.yaml
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-agent/files
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-operator/files
        envsubst '${CILIUM_IMAGE_REGISTRY},${CILIUM_VERSION_TAG}' < test/integration/manifests/cilium/v${DIR}/cilium-agent/templates/daemonset.yaml | kubectl apply -f -
        envsubst '${CILIUM_IMAGE_REGISTRY},${CILIUM_VERSION_TAG}' < test/integration/manifests/cilium/v${DIR}/cilium-operator/templates/deployment.yaml | kubectl apply -f -
        # Use different file directories for nightly and current cilium version

  - template: ../../templates/cilium-cli.yaml

  - script: |
      echo "Start Azilium E2E Tests on Overlay Cluster"
      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]
      then
          echo "Running nightly"
      else
          CNS=$(make cns-version) IPAM=$(make azure-ipam-version)
      fi
      sudo -E env "PATH=$PATH" make test-load AZURE_IPAM_VERSION=${IPAM} CNS_VERSION=${CNS}
    retryCountOnTaskFailure: 3
    displayName: "Run Azilium E2E on AKS Overlay"
    env:
      CNS: $(CNS_VERSION)
      IPAM: $(AZURE_IPAM_VERSION)
      SCALE_UP: 32
      OS_TYPE: linux
      VALIDATE_STATEFILE: true
      INSTALL_CNS: true
      INSTALL_OVERLAY: true
      CLEANUP: true

  - script: |
      kubectl get pods -A
      echo "Waiting < 2 minutes for cilium to be ready"
      # Ensure Cilium is ready Xm\Xs
      cilium status --wait --wait-duration 2m
    retryCountOnTaskFailure: 3
    displayName: "Cilium Status"

  - task: AzureCLI@2
    displayName: "Restart Nodes"
    env:
      AZCLI: az
      CLUSTER: ${{ parameters.clusterName }}
      SCALE_UP: ${{ coalesce(parameters.scaleup, 32) }}
      RESOURCE_GROUP: "MC_${{ parameters.clusterName }}_${{ parameters.clusterName }}_$(REGION_AKS_CLUSTER_TEST)"
      REGION: $(REGION_AKS_CLUSTER_TEST)
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        kubectl get po -owide -A
        echo "Restarting nodes"
        for val in $(az vmss list -g "$RESOURCE_GROUP" --query "[].name" -o tsv); do
          make -C ./hack/aks restart-vmss VMSS_NAME=${val}
        done

  - task: AzureCLI@2
    displayName: "Validate Node Restart"
    retryCountOnTaskFailure: 3
    env:
      ITERATIONS: 2
      SCALE_UP: ${{ parameters.scaleup }}
      OS_TYPE: linux
      RESTART_CASE: true
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        cd test/integration/load

        # Scale Cluster Up/Down to confirm functioning CNS
        go test -count 1 -timeout 30m -tags load -run ^TestLoad$
        kubectl get pods -owide -A

        cd ../../..
        echo "Validating Node Restart"
        make test-validate-state
        kubectl delete ns load-test

  - script: |
      set -e
      echo "Run Cilium Connectivity Tests"
      cilium status
      cilium connectivity test --connect-timeout 4s --request-timeout 30s --test '!pod-to-pod-encryption,!node-to-node-encryption' --force-deploy
      ns=`kubectl get ns | grep cilium-test | awk '{print $1}'`
      echo "##vso[task.setvariable variable=ciliumNamespace]$ns"
    retryCountOnTaskFailure: 3
    displayName: "Run Cilium Connectivity Tests"

  - ${{ if eq( parameters['testHubble'], true) }}:
      - script: |
          echo "enable Hubble metrics server"
          kubectl apply -f test/integration/manifests/cilium/hubble/hubble-peer-svc.yaml
          kubectl apply -f test/integration/manifests/cilium/v1.14.4/cilium-config/cilium-config-hubble.yaml
          kubectl rollout restart ds cilium -n kube-system
          echo "wait <3 minutes for pods to be ready after restart"
          kubectl rollout status ds cilium -n kube-system --timeout=3m
          kubectl get pods -Aowide
          echo "verify Hubble metrics endpoint is usable"
          go test ./test/integration/networkobservability -v -tags=networkobservability
        retryCountOnTaskFailure: 3
        displayName: "Run Hubble Connectivity Tests"

  - script: |
      set -e
      echo "validate pod IP assignment and check systemd-networkd restart"
      kubectl get pod -owide -A

      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]; then
        echo "Check cilium identities in "$CILIUM_NS" namepsace during nightly run"
        echo "expect the identities to be deleted when the namespace is deleted"
        kubectl get ciliumidentity | grep cilium-test
      fi
      make test-validate-state
      echo "delete cilium connectivity test resources and re-validate state" # TODO Delete this and the next 4 lines if connectivity no longer has bug
      kubectl delete ns "$CILIUM_NS"
      kubectl get pod -owide -A
      make test-validate-state
    displayName: "Validate Pods"
    env:
      CILIUM_NS: $(ciliumNamespace)
      CNI_TYPE: cilium_dualstack

  - script: |
      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]; then
        kubectl get pod -owide -n "$CILIUM_NS"
        echo "wait for pod and cilium identity deletion in cilium-test namespace"
        while true; do
          pods=$(kubectl get pods -n "$CILIUM_NS" --no-headers=true 2>/dev/null)
          if [[ -z "$pods" ]]; then
            echo "No pods found"
              break
          fi
          sleep 2s
        done
        sleep 20s
        echo "Verify cilium identities are deleted from cilium-test"
        checkIdentity="$(kubectl get ciliumidentity -o json | grep cilium-test | jq -e 'length == 0')"
        if [[ -n $checkIdentity ]]; then
          echo "##[error]Cilium Identities still present in "$CILIUM_NS" namespace"
          exit 1
        else
          printf -- "Identities deleted from "$CILIUM_NS" namespace\n"
        fi
      else
        echo "skip cilium identities check for PR pipeline"
      fi
    displayName: "Verify Cilium Identities Deletion"
    env:
      CILIUM_NS: $(ciliumNamespace)

  - script: |
      echo "Run wireserver and metadata connectivity Tests"
      bash test/network/wireserver_metadata_test.sh
    retryCountOnTaskFailure: 3
    displayName: "Run Wireserver and Metadata Connectivity Tests"

  - script: |
      cd hack/scripts
      chmod +x async-delete-test.sh
      ./async-delete-test.sh
      if ! [ -z $(kubectl -n kube-system get ds  azure-cns | grep non-existing) ]; then
        kubectl -n kube-system patch daemonset azure-cns --type json -p='[{"op": "remove", "path": "/spec/template/spec/nodeSelector/non-existing"}]'
      fi
    displayName: "Verify Async Delete when CNS is down"
