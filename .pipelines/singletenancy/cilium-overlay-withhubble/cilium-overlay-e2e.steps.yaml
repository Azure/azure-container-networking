parameters:
  name: ""
  clusterName: ""
  testHubble: false
  scaleup: ""

steps:

  - bash: |
      go version
      go env
      mkdir -p '$(GOBIN)'
      mkdir -p '$(GOPATH)/pkg'
      mkdir -p '$(modulePath)'
      echo '##vso[task.prependpath]$(GOBIN)'
      echo '##vso[task.prependpath]$(GOROOT)/bin'
    name: "GoEnv"
    displayName: "Set up the Go environment"

  - task: KubectlInstaller@0
    inputs:
      kubectlVersion: latest

  - task: AzureCLI@2
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}
        ls -lah
        export CILIUM_VERSION_TAG=${CILIUM_HUBBLE_VERSION_TAG}
        export DIR=$(echo ${CILIUM_VERSION_TAG#v} | cut -d. -f1,2)
        echo "installing files from ${DIR}"
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-config/cilium-config-hubble.yaml
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-agent/files
        kubectl apply -f test/integration/manifests/cilium/v${DIR}/cilium-operator/files
        envsubst '${CILIUM_IMAGE_REGISTRY},${CILIUM_VERSION_TAG}' < test/integration/manifests/cilium/v${DIR}/cilium-agent/templates/daemonset.yaml | kubectl apply -f -
        envsubst '${CILIUM_IMAGE_REGISTRY},${CILIUM_VERSION_TAG}' < test/integration/manifests/cilium/v${DIR}/cilium-operator/templates/deployment.yaml | kubectl apply -f -
        # Use different file directories for nightly and current cilium version
    name: "installCilium"
    displayName: "Install Cilium on AKS Overlay"

  - template: ../../templates/cilium-cli.yaml

  - script: |
      echo "Start Azilium E2E Tests on Overlay Cluster"
      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]
      then
          CNS=$(CNS_VERSION) IPAM=$(AZURE_IPAM_VERSION) && echo "Running nightly"
      else
          CNS=$(make cns-version) IPAM=$(make azure-ipam-version)
      fi
      sudo -E env "PATH=$PATH" make test-load \
        SCALE_UP=32 OS_TYPE=linux VALIDATE_STATEFILE=true \
        INSTALL_CNS=true INSTALL_OVERLAY=true CLEANUP=true \
        AZURE_IPAM_VERSION=$(AZURE_IPAM_VERSION) CNS_VERSION=$(CNS_VERSION) \
        IPAM_IMAGE_NAME_OVERRIDE=$(IPAM_IMAGE_NAME_OVERRIDE) CNS_IMAGE_NAME_OVERRIDE=$(CNS_IMAGE_NAME_OVERRIDE)
    retryCountOnTaskFailure: 3
    name: "aziliumTest"
    displayName: "Run Azilium E2E on AKS Overlay"

  - script: |
      kubectl get pods -A
      echo "Waiting < 2 minutes for cilium to be ready"
      # Ensure Cilium is ready Xm\Xs
      cilium status --wait --wait-duration 2m
      kubectl get crd -A
    retryCountOnTaskFailure: 3
    name: "CiliumStatus"
    displayName: "Cilium Status"

  - task: AzureCLI@2
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        kubectl get po -owide -A
        clusterName=${{ parameters.clusterName }}
        echo "Restarting nodes"
        for val in $(az vmss list -g MC_${clusterName}_${clusterName}_$(REGION_AKS_CLUSTER_TEST) --query "[].name" -o tsv); do
          make -C ./hack/aks restart-vmss AZCLI=az CLUSTER=${clusterName} REGION=$(REGION_AKS_CLUSTER_TEST) VMSS_NAME=${val}
        done
    displayName: "Restart Nodes"

  - task: AzureCLI@2
    inputs:
      azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
      scriptLocation: "inlineScript"
      scriptType: "bash"
      addSpnToEnvironment: true
      inlineScript: |
        set -e
        cd test/integration/load

        # Scale Cluster Up/Down to confirm functioning CNS
        ITERATIONS=2 SCALE_UP=${{ parameters.scaleup }} OS_TYPE=linux go test -count 1 -timeout 30m -tags load -run ^TestLoad$
        kubectl get pods -owide -A

        cd ../../..
        echo "Validating Node Restart"
        make test-validate-state OS_TYPE=linux RESTART_CASE=true
        kubectl delete ns load-test
    displayName: "Validate Node Restart"
    retryCountOnTaskFailure: 3

  - script: |
      set -e
      echo "Run Cilium Connectivity Tests"
      cilium status
      cilium connectivity test --connect-timeout 4s --request-timeout 30s --test '!pod-to-pod-encryption,!node-to-node-encryption,!check-log-errors' --force-deploy
      ns=`kubectl get ns | grep cilium-test | awk '{print $1}'`
      echo "##vso[task.setvariable variable=ciliumNamespace]$ns"
    retryCountOnTaskFailure: 3
    name: "ciliumConnectivityTests"
    displayName: "Run Cilium Connectivity Tests"

  - ${{ if eq( parameters['testHubble'], true) }}:
      - script: |
          echo "enable Hubble metrics server"
          kubectl apply -f test/integration/manifests/cilium/hubble/hubble-peer-svc.yaml
          kubectl apply -f test/integration/manifests/cilium/v1.14.4/cilium-config/cilium-config-hubble.yaml
          kubectl rollout restart ds cilium -n kube-system
          echo "wait <3 minutes for pods to be ready after restart"
          kubectl rollout status ds cilium -n kube-system --timeout=3m
          kubectl get pods -Aowide
          echo "verify Hubble metrics endpoint is usable"
          go test ./test/integration/networkobservability -v -tags=networkobservability
        retryCountOnTaskFailure: 3
        name: "HubbleConnectivityTests"
        displayName: "Run Hubble Connectivity Tests"

  - script: |
      set -e
      echo "validate pod IP assignment and check systemd-networkd restart"
      kubectl get pod -owide -A

      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]; then
        echo "Check cilium identities in $(ciliumNamespace) namepsace during nightly run"
        echo "expect the identities to be deleted when the namespace is deleted"
        kubectl get ciliumidentity | grep cilium-test
      fi
      make test-validate-state
      echo "delete cilium connectivity test resources and re-validate state" # TODO Delete this and the next 4 lines if connectivity no longer has bug
      kubectl delete ns $(ciliumNamespace)
      kubectl get pod -owide -A
      make test-validate-state
    name: "validatePods"
    displayName: "Validate Pods"

  - script: |
      if [ "$CILIUM_VERSION_TAG" = "cilium-nightly-pipeline" ]; then
        kubectl get pod -owide -n $(ciliumNamespace)
        echo "wait for pod and cilium identity deletion in cilium-test namespace"
        while true; do
          pods=$(kubectl get pods -n $(ciliumNamespace) --no-headers=true 2>/dev/null)
          if [[ -z "$pods" ]]; then
            echo "No pods found"
              break
          fi
          sleep 2s
        done
        sleep 20s
        echo "Verify cilium identities are deleted from cilium-test"
        checkIdentity="$(kubectl get ciliumidentity -o json | grep cilium-test | jq -e 'length == 0')"
        if [[ -n $checkIdentity ]]; then
          echo "##[error]Cilium Identities still present in $(ciliumNamespace) namespace"
          exit 1
        else
          printf -- "Identities deleted from $(ciliumNamespace) namespace\n"
        fi
      else
        echo "skip cilium identities check for PR pipeline"
      fi
    name: "CiliumIdentities"
    displayName: "Verify Cilium Identities Deletion"

  - script: |
      echo "Run wireserver and metadata connectivity Tests"
      bash test/network/wireserver_metadata_test.sh
    retryCountOnTaskFailure: 3
    name: "WireserverMetadataConnectivityTests"
    displayName: "Run Wireserver and Metadata Connectivity Tests"

  - script: |
      cd hack/scripts
      chmod +x async-delete-test.sh
      ./async-delete-test.sh
      if ! [ -z $(kubectl -n kube-system get ds  azure-cns | grep non-existing) ]; then
        kubectl -n kube-system patch daemonset azure-cns --type json -p='[{"op": "remove", "path": "/spec/template/spec/nodeSelector/non-existing"}]'
      fi
    name: "testAsyncDelete"
    displayName: "Verify Async Delete when CNS is down"
  
  - template: ../../templates/cilium-mtu-check.yaml

  - script: |
      ARTIFACT_DIR=$(Build.ArtifactStagingDirectory)/test-output/
      echo $ARTIFACT_DIR
      sudo rm -rf $ARTIFACT_DIR
      sudo rm -rf test/integration/logs
    name: "Cleanupartifactdir"
    displayName: "Cleanup artifact dir"
    condition: always()
